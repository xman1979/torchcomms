// Copyright (c) Meta Platforms, Inc. and affiliates.

#pragma once
#include "comms/ctran/algos/AllToAll/Types.h"
#include "comms/ctran/algos/CtranAlgoDev.h"
#include "comms/ctran/algos/DevAlgoImpl.cuh"
#include "comms/ctran/algos/DevCommon.cuh"
#include "comms/ctran/gpe/CtranGpeDev.h"

template <typename T>
__device__ void prepareBcastArg(
    KernelElem* elemH,
    ctran::alltoalldedup::KernelArgs& args,
    CtranAlgoDevBcastArg& bcastArg) {
  bcastArg.src = elemH->bcast.src;
  bcastArg.count = elemH->bcast.count;
  // need barrier to ensure all peers have finished update to the local dst
  // before kernel finishes
  bcastArg.barrier = true;
  // No host side access before kernel terminates; thus skip flush
  bcastArg.flushMem = false;
  bcastArg.nvectors = statex->nLocalRanks();

  // TODO: recv buffs are static since persistent collective, so pass in
  // pointers instead and compute offset
  //  H2D load dsts that has to be specified by GPE thread
  loadAlgoDevVecPtr(bcastArg.dsts, elemH->bcast.dsts, bcastArg.nvectors);
}

template <typename T>
static __device__ __forceinline__ void bcastOnPost(
    KernelElem* elemH,
    ctran::alltoalldedup::KernelArgs& args,
    const int numBlocksPerBcast,
    int bcastBlockIdx) {
  bool revoked = false;
  elemWaitPostOrRevokeByGroup(elemH, bcastBlockIdx, &revoked);

  // Load arguments from host-pinned memory before executing bcast
  CtranAlgoDevBcastArg bcastArg;
  prepareBcastArg<T>(elemH, args, bcastArg);

  // Choose kMultiPutBcast since GPE thread ensures all ranks have joined and
  // the internal peer shift can avoid incast congestion in multiPut
  ctranKernBcast<T, kMultiPutBcast>(
      bcastArg, elemH, bcastBlockIdx, numBlocksPerBcast, 0);

  elemCompleteByGroup(elemH, bcastBlockIdx);
}

template <typename T>
__global__ void ncclKernelAllToAllDedup(
    int* flag,
    CtranAlgoDeviceState* devState,
    ctran::alltoalldedup::KernelArgs args) {
  const auto gtIdx = blockDim.x * blockIdx.x + threadIdx.x;

  if (flag && gtIdx == 0) {
    ctran::device::devLoadAbortFlags(flag, devState);
    ctran::device::KernelStartGpe(flag);
  }

  devStateLoadToShm(devState);

  const auto numBlocks = gridDim.x;
  const auto numBcasts = args.numIbPeers;
  const auto numBlocksPerBcast = numBlocks / numBcasts;
  const auto bcastGroup = blockIdx.x / numBlocksPerBcast;
  const auto bcastBlockIdx = blockIdx.x % numBlocksPerBcast;

  // get KernelElem associated with bcastGroup
  KernelElem* curElem = args.bcastElemList;
  for (int i = 0; i < bcastGroup; i++) {
    curElem = curElem->next;
  }

  bcastOnPost<T>(curElem, args, numBlocksPerBcast, bcastBlockIdx);

  if (flag && gtIdx == 0) {
    ctran::device::KernelWaitGpeTerminate(flag);
  }
}

#define DECL_CTRAN_ALLTOALLDEDUP_KERN(T)               \
  template __global__ void ncclKernelAllToAllDedup<T>( \
      int* flag,                                       \
      CtranAlgoDeviceState* devState,                  \
      ctran::alltoalldedup::KernelArgs args)
